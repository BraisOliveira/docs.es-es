---
title: Métricas de ML.NET
description: Introducción a las métricas que se utilizan para evaluar el rendimiento de un modelo de ML.NET
ms.date: 04/29/2019
author: ''
ms.openlocfilehash: 802f0a8fd32c492c8d9f89933b183802cb178cb3
ms.sourcegitcommit: 7e129d879ddb42a8b4334eee35727afe3d437952
ms.translationtype: HT
ms.contentlocale: es-ES
ms.lasthandoff: 05/23/2019
ms.locfileid: "66053040"
---
# <a name="model-evaluation-metrics-in-mlnet"></a><span data-ttu-id="f31bf-103">Métricas de evaluación de modelos en ML.NET</span><span class="sxs-lookup"><span data-stu-id="f31bf-103">Model evaluation metrics in ML.NET</span></span>

## <a name="metrics-for-binary-classification"></a><span data-ttu-id="f31bf-104">Métricas para la clasificación binaria</span><span class="sxs-lookup"><span data-stu-id="f31bf-104">Metrics for Binary Classification</span></span>

| <span data-ttu-id="f31bf-105">Métricas</span><span class="sxs-lookup"><span data-stu-id="f31bf-105">Metrics</span></span>   |      <span data-ttu-id="f31bf-106">Descripción</span><span class="sxs-lookup"><span data-stu-id="f31bf-106">Description</span></span>      |  <span data-ttu-id="f31bf-107">Buscar</span><span class="sxs-lookup"><span data-stu-id="f31bf-107">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="f31bf-108">**Precisión**</span><span class="sxs-lookup"><span data-stu-id="f31bf-108">**Accuracy**</span></span> |  <span data-ttu-id="f31bf-109">La [precisión](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) es la proporción de predicciones correctas con un conjunto de datos de prueba.</span><span class="sxs-lookup"><span data-stu-id="f31bf-109">[Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification) is the proportion of correct predictions with a test data set.</span></span> <span data-ttu-id="f31bf-110">Es la relación entre el número de predicciones correctas y el número total de ejemplos de entrada.</span><span class="sxs-lookup"><span data-stu-id="f31bf-110">It is the ratio of number of correct predictions to the total number of input samples.</span></span> <span data-ttu-id="f31bf-111">Funciona bien solo si hay un número similar de muestras que pertenecen a cada clase.</span><span class="sxs-lookup"><span data-stu-id="f31bf-111">It works well only if there are similar number of samples belonging to each class.</span></span>| <span data-ttu-id="f31bf-112">**Cuanto más cerca de 1,00, mejor**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-112">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="f31bf-113">Pero exactamente 1,00 indica un problema (normalmente: fuga de etiqueta/destino, sobreajuste o pruebas con datos de entrenamiento).</span><span class="sxs-lookup"><span data-stu-id="f31bf-113">But exactly 1.00 indicates an issue (commonly: label/target leakage, over-fitting, or testing with training data).</span></span> <span data-ttu-id="f31bf-114">Cuando los datos de prueba están desequilibrados (donde la mayoría de las instancias pertenece a una de las clases), el conjunto de datos es muy pequeño o las puntuaciones se acercan a 0,00 o 1,00, la precisión no captura realmente la eficacia de un clasificador y es necesario comprobar métricas adicionales.</span><span class="sxs-lookup"><span data-stu-id="f31bf-114">When the test data is unbalanced (where most of the instances belong to one of the classes), the dataset is very small, or scores approach 0.00 or 1.00, then accuracy doesn’t really capture the effectiveness of a classifier and you need to check additional metrics.</span></span> |
| <span data-ttu-id="f31bf-115">**AUC**</span><span class="sxs-lookup"><span data-stu-id="f31bf-115">**AUC**</span></span> |    <span data-ttu-id="f31bf-116">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) o *Área bajo la curva*: Esto mide el área bajo la curva que se creó limpiando la tasa de positivos verdaderos frente a la tasa de falsos positivos.</span><span class="sxs-lookup"><span data-stu-id="f31bf-116">[aucROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) or *Area under the curve*: This is measuring the area under the curve created by sweeping the true positive rate vs. the false positive rate.</span></span>  |   <span data-ttu-id="f31bf-117">**Cuanto más cerca de 1,00, mejor**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-117">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="f31bf-118">Para que un modelo sea aceptable, debe ser superior a 0,50. Un modelo con AUC de 0,50 o menos no tiene ningún valor.</span><span class="sxs-lookup"><span data-stu-id="f31bf-118">It should be greater than 0.50 for a model to be acceptable; a model with AUC of 0.50 or less is worthless.</span></span> |
| <span data-ttu-id="f31bf-119">**AUCPR**</span><span class="sxs-lookup"><span data-stu-id="f31bf-119">**AUCPR**</span></span> | <span data-ttu-id="f31bf-120">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) o *Área bajo la curva de una curva de precisión-recuperación*: Medida útil de éxito de predicción cuando las clases están muy poco equilibradas (conjuntos de datos muy sesgados).</span><span class="sxs-lookup"><span data-stu-id="f31bf-120">[aucPR](https://www.coursera.org/lecture/ml-classification/precision-recall-curve-rENu8) or *Area under the curve of a Precision-Recall curve*: Useful measure of success of prediction when the classes are very imbalanced (highly skewed datasets).</span></span> |  <span data-ttu-id="f31bf-121">**Cuanto más cerca de 1,00, mejor**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-121">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="f31bf-122">Las puntuaciones altas cercanas a 1,00 muestran que el clasificador devuelve resultados precisos (alta precisión), así como una mayoría de todos los resultados positivos (recuperación alta).</span><span class="sxs-lookup"><span data-stu-id="f31bf-122">High scores close to 1.00 show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</span></span> |
| <span data-ttu-id="f31bf-123">**Puntuación F1**</span><span class="sxs-lookup"><span data-stu-id="f31bf-123">**F1-score**</span></span> | <span data-ttu-id="f31bf-124">La [puntuación F1](https://en.wikipedia.org/wiki/F1_score) también se denomina *puntuación F equilibrada o F medida F*.</span><span class="sxs-lookup"><span data-stu-id="f31bf-124">[F1 score](https://en.wikipedia.org/wiki/F1_score) also known as *balanced F-score or F-measure*.</span></span> <span data-ttu-id="f31bf-125">Es la media armónica de la precisión y la recuperación.</span><span class="sxs-lookup"><span data-stu-id="f31bf-125">It's the harmonic mean of the precision and recall.</span></span> <span data-ttu-id="f31bf-126">La puntuación F1 resulta útil cuando desea buscar un equilibrio entre la precisión y la recuperación.</span><span class="sxs-lookup"><span data-stu-id="f31bf-126">F1 Score is helpful when you want to seek a balance between Precision and Recall.</span></span>| <span data-ttu-id="f31bf-127">**Cuanto más cerca de 1,00, mejor**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-127">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="f31bf-128">Una puntuación F1 alcanza el mejor valor en 1,00 y la peor puntuación en 0,00.</span><span class="sxs-lookup"><span data-stu-id="f31bf-128">An F1 score reaches its best value at 1.00 and worst score at 0.00.</span></span> <span data-ttu-id="f31bf-129">Indica cuán preciso es el clasificador.</span><span class="sxs-lookup"><span data-stu-id="f31bf-129">It tells you how precise your classifier is.</span></span> |

<span data-ttu-id="f31bf-130">Para obtener más información sobre las métricas de clasificación binaria, lea los artículos siguientes:</span><span class="sxs-lookup"><span data-stu-id="f31bf-130">For further details on binary classification metrics read the following articles:</span></span>

- [<span data-ttu-id="f31bf-131">¿Exactitud, precisión, recuperación o F1?</span><span class="sxs-lookup"><span data-stu-id="f31bf-131">Accuracy, Precision, Recall or F1?</span></span>](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9)
- [<span data-ttu-id="f31bf-132">Clase de métricas para la clasificación binaria</span><span class="sxs-lookup"><span data-stu-id="f31bf-132">Binary Classification Metrics class</span></span>](xref:Microsoft.ML.Data.BinaryClassificationMetrics)
- [<span data-ttu-id="f31bf-133">La relación entre precisión-recuperación y curvas de ROC</span><span class="sxs-lookup"><span data-stu-id="f31bf-133">The Relationship Between Precision-Recall and ROC Curves</span></span>](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf)

## <a name="metrics-for-multi-class-classification"></a><span data-ttu-id="f31bf-134">Métricas para la clasificación multiclase</span><span class="sxs-lookup"><span data-stu-id="f31bf-134">Metrics for Multi-class Classification</span></span>

| <span data-ttu-id="f31bf-135">Métricas</span><span class="sxs-lookup"><span data-stu-id="f31bf-135">Metrics</span></span>   |      <span data-ttu-id="f31bf-136">Descripción</span><span class="sxs-lookup"><span data-stu-id="f31bf-136">Description</span></span>      |  <span data-ttu-id="f31bf-137">Buscar</span><span class="sxs-lookup"><span data-stu-id="f31bf-137">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="f31bf-138">**Microprecisión**</span><span class="sxs-lookup"><span data-stu-id="f31bf-138">**Micro-Accuracy**</span></span> |  <span data-ttu-id="f31bf-139">La [precisión de micropromedio](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MicroAccuracy) agrega las contribuciones de todas las clases para calcular la métrica promedio.</span><span class="sxs-lookup"><span data-stu-id="f31bf-139">[Micro-average Accuracy](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MicroAccuracy) aggregates the contributions of all classes to compute the average metric.</span></span> <span data-ttu-id="f31bf-140">Es la fracción de instancias que se predijeron correctamente.</span><span class="sxs-lookup"><span data-stu-id="f31bf-140">It is the fraction of instances predicted correctly.</span></span> <span data-ttu-id="f31bf-141">El micropromedio no tiene en cuenta la pertenencia a una clase.</span><span class="sxs-lookup"><span data-stu-id="f31bf-141">The micro-average does not take class membership into account.</span></span> <span data-ttu-id="f31bf-142">Básicamente, todos los pares de ejemplo y clase contribuyen del mismo modo a la métrica de precisión.</span><span class="sxs-lookup"><span data-stu-id="f31bf-142">Basically, every sample-class pair contributes equally to the accuracy metric.</span></span> | <span data-ttu-id="f31bf-143">**Cuanto más cerca de 1,00, mejor**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-143">**The closer to 1.00, the better**.</span></span> <span data-ttu-id="f31bf-144">En una tarea de clasificación multiclase, la microprecisión es preferible a la macroprecisión si se sospecha que podría haber un desequilibrio de clases (por ejemplo,</span><span class="sxs-lookup"><span data-stu-id="f31bf-144">In a multi-class classification task, micro-accuracy is preferable over macro-accuracy if you suspect there might be class imbalance (i.e</span></span> <span data-ttu-id="f31bf-145">podría tener muchos más ejemplos de una clase que de otras clases).</span><span class="sxs-lookup"><span data-stu-id="f31bf-145">you may have many more examples of one class than of other classes).</span></span>|
| <span data-ttu-id="f31bf-146">**Macroprecisión**</span><span class="sxs-lookup"><span data-stu-id="f31bf-146">**Macro-Accuracy**</span></span> | <span data-ttu-id="f31bf-147">La [precisión de macropromedio](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MacroAccuracy) es la precisión promedio en el nivel de clase.</span><span class="sxs-lookup"><span data-stu-id="f31bf-147">[Macro-average Accuracy](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.MacroAccuracy) is the average accuracy at the class level.</span></span> <span data-ttu-id="f31bf-148">La precisión de cada clase se calcula y la macroprecisión es el promedio de estas precisiones.</span><span class="sxs-lookup"><span data-stu-id="f31bf-148">The accuracy for each class is computed and the macro-accuracy is the average of these accuracies.</span></span> <span data-ttu-id="f31bf-149">Básicamente, todas las clases contribuyen del mismo modo a la métrica de precisión.</span><span class="sxs-lookup"><span data-stu-id="f31bf-149">Basically, every class contributes equally to the accuracy metric.</span></span> <span data-ttu-id="f31bf-150">Las clases minoritarias tienen el mismo peso que las clases más grandes.</span><span class="sxs-lookup"><span data-stu-id="f31bf-150">Minority classes are given equal weight as the larger classes.</span></span> <span data-ttu-id="f31bf-151">La métrica de macropromedio proporciona el mismo peso a cada clase, independientemente de cuántas instancias de esa clase contiene el conjunto de datos.</span><span class="sxs-lookup"><span data-stu-id="f31bf-151">The macro-average metric gives the same weight to each class, no matter how many instances from that class the dataset contains.</span></span> |  <span data-ttu-id="f31bf-152">**Cuanto más cerca de 1,00, mejor**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-152">**The closer to 1.00, the better**.</span></span>  <span data-ttu-id="f31bf-153">Calcula la métrica de forma independiente para cada clase y, a continuación, toma la media (por lo tanto, se consideran todas las clases de igual forma)</span><span class="sxs-lookup"><span data-stu-id="f31bf-153">It computes the metric independently for each class and then takes the average (hence treating all classes equally)</span></span> |
| <span data-ttu-id="f31bf-154">**Pérdida de registro**</span><span class="sxs-lookup"><span data-stu-id="f31bf-154">**Log-loss**</span></span>| <span data-ttu-id="f31bf-155">La [pérdida logarítmica](http://wiki.fast.ai/index.php/Log_Loss) mide el rendimiento de un modelo de clasificación donde la entrada de predicción es un valor de probabilidad de entre 0,00 y 1,00.</span><span class="sxs-lookup"><span data-stu-id="f31bf-155">[Logarithmic loss](http://wiki.fast.ai/index.php/Log_Loss) measures the performance of a classification model where the prediction input is a probability value between 0.00 and 1.00.</span></span> <span data-ttu-id="f31bf-156">La pérdida de registro aumenta a medida que la probabilidad de predicción difiere de la etiqueta real.</span><span class="sxs-lookup"><span data-stu-id="f31bf-156">Log-loss increases as the predicted probability diverges from the actual label.</span></span> | <span data-ttu-id="f31bf-157">**Cuanto más cerca de 0,00, mejor**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-157">**The closer to 0.00, the better**.</span></span> <span data-ttu-id="f31bf-158">Un modelo perfecto tendría una pérdida de registro de 0,00.</span><span class="sxs-lookup"><span data-stu-id="f31bf-158">A perfect model would have a log-loss of 0.00.</span></span> <span data-ttu-id="f31bf-159">El objetivo de nuestros modelos de Machine Learning es minimizar este valor.</span><span class="sxs-lookup"><span data-stu-id="f31bf-159">The goal of our machine learning models is to minimize this value.</span></span>|
| <span data-ttu-id="f31bf-160">**Reducción de pérdida logarítmica**</span><span class="sxs-lookup"><span data-stu-id="f31bf-160">**Log-Loss Reduction**</span></span> | <span data-ttu-id="f31bf-161">La [reducción de pérdida logarítmica](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.LogLossReduction) se puede interpretar como la ventaja del clasificador sobre una predicción aleatoria.</span><span class="sxs-lookup"><span data-stu-id="f31bf-161">[Logarithmic loss reduction](xref:Microsoft.ML.Data.MulticlassClassificationMetrics.LogLossReduction) can be interpreted as the advantage of the classifier over a random prediction.</span></span>| <span data-ttu-id="f31bf-162">**Parte de -inf y 1,00, donde 1,00 equivale a una predicción perfecta, y 0,00 indica una predicción aproximada**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-162">**Ranges from -inf and 1.00, where 1.00 is perfect predictions and 0.00 indicates mean predictions**.</span></span> <span data-ttu-id="f31bf-163">Por ejemplo, si el valor equivale a 0,20, se puede interpretar como "la probabilidad de que una predicción correcta sea 20 % mejor que el cálculo aleatorio"</span><span class="sxs-lookup"><span data-stu-id="f31bf-163">For example, if the value equals 0.20, it can be interpreted as "the probability of a correct prediction is 20% better than random guessing"</span></span>|

<span data-ttu-id="f31bf-164">Por lo general, la microprecisión se alinea mejor con las necesidades empresariales de predicciones de ML.</span><span class="sxs-lookup"><span data-stu-id="f31bf-164">Micro-accuracy is generally better aligned with the business needs of ML predictions.</span></span> <span data-ttu-id="f31bf-165">Si desea seleccionar una sola métrica para elegir la calidad de una tarea de clasificación multiclase, normalmente debería ser la de microprecisión.</span><span class="sxs-lookup"><span data-stu-id="f31bf-165">If you want to select a single metric for choosing the quality of a multiclass classification task, it should usually be micro-accuracy.</span></span>

<span data-ttu-id="f31bf-166">Ejemplo, para una tarea de clasificación de incidencias de soporte técnico: (asigna incidencias entrantes a los equipos de soporte técnico)</span><span class="sxs-lookup"><span data-stu-id="f31bf-166">Example, for a support ticket classification task: (maps incoming tickets to support teams)</span></span>

- <span data-ttu-id="f31bf-167">Microprecisión: ¿con qué frecuencia se clasifica una incidencia entrante en el equipo adecuado?</span><span class="sxs-lookup"><span data-stu-id="f31bf-167">Micro-accuracy -- how often does an incoming ticket get classified to the right team?</span></span>
- <span data-ttu-id="f31bf-168">Macroprecisión: para un equipo promedio, ¿con qué frecuencia es correcta una incidencia entrante para su equipo?</span><span class="sxs-lookup"><span data-stu-id="f31bf-168">Macro-accuracy -- for an average team, how often is an incoming ticket correct for their team?</span></span>

<span data-ttu-id="f31bf-169">La macroprecisión proporciona más peso a los equipos pequeños en este ejemplo: un equipo pequeño que obtiene solo 10 incidencias al año cuenta tanto como un equipo grande con 10 000 incidencias al año.</span><span class="sxs-lookup"><span data-stu-id="f31bf-169">Macro-accuracy overweights small teams in this example; a small team which gets only 10 tickets per year counts as much as a large team with 10k tickets per year.</span></span> <span data-ttu-id="f31bf-170">En este caso, la microprecisión se correlaciona mejor con la necesidad empresarial que se pregunta "cuánto tiempo y dinero puede ahorrar la compañía automatizando mi proceso de enrutamiento de incidencias".</span><span class="sxs-lookup"><span data-stu-id="f31bf-170">Micro-accuracy in this case correlates better with the business need of, "how much time/money can the company save by automating my ticket routing process".</span></span>

<span data-ttu-id="f31bf-171">Para obtener más información sobre las métricas de clasificación multiclase, lea los artículos siguientes:</span><span class="sxs-lookup"><span data-stu-id="f31bf-171">For further details on multi-class classification metrics read the following articles:</span></span>

- [<span data-ttu-id="f31bf-172">Micropromedio y macropromedio de precisión, recuperación y puntuación F</span><span class="sxs-lookup"><span data-stu-id="f31bf-172">Micro- and Macro-average of Precision, Recall and F-Score</span></span>](http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html)
- [<span data-ttu-id="f31bf-173">Clasificación multiclase con un conjunto de datos desequilibrado</span><span class="sxs-lookup"><span data-stu-id="f31bf-173">Multiclass Classification with Imbalanced Dataset</span></span>](https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a)

## <a name="metrics-for-regression"></a><span data-ttu-id="f31bf-174">Métricas de regresión</span><span class="sxs-lookup"><span data-stu-id="f31bf-174">Metrics for Regression</span></span>

| <span data-ttu-id="f31bf-175">Métricas</span><span class="sxs-lookup"><span data-stu-id="f31bf-175">Metrics</span></span>   |      <span data-ttu-id="f31bf-176">Descripción</span><span class="sxs-lookup"><span data-stu-id="f31bf-176">Description</span></span>      |  <span data-ttu-id="f31bf-177">Buscar</span><span class="sxs-lookup"><span data-stu-id="f31bf-177">Look for</span></span> |
|-----------|-----------------------|-----------|
| <span data-ttu-id="f31bf-178">**R cuadrado**</span><span class="sxs-lookup"><span data-stu-id="f31bf-178">**R-Squared**</span></span> |  <span data-ttu-id="f31bf-179">[R cuadrado (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination), o el *coeficiente de determinación* representan la eficacia predictiva del modelo como un valor comprendido entre -inf y 1,00.</span><span class="sxs-lookup"><span data-stu-id="f31bf-179">[R-squared (R2)](https://en.wikipedia.org/wiki/Coefficient_of_determination), or *Coefficient of determination* represents the predictive power of the model as a value between -inf and 1.00.</span></span> <span data-ttu-id="f31bf-180">1,00 significa que hay un ajuste perfecto y, dado que el ajuste puede ser arbitrariamente deficiente, las puntuaciones pueden ser negativas.</span><span class="sxs-lookup"><span data-stu-id="f31bf-180">1.00 means there is a perfect fit, and the fit can be arbitrarly poor so the scores can be negative.</span></span> <span data-ttu-id="f31bf-181">Una puntuación de 0,00 significa que el modelo consiste en adivinar el valor esperado para la etiqueta.</span><span class="sxs-lookup"><span data-stu-id="f31bf-181">A score of 0.00 means the model is guessing the expected value for the label.</span></span> <span data-ttu-id="f31bf-182">R2 mide la proximidad de los valores de datos de prueba reales a los valores de predicción.</span><span class="sxs-lookup"><span data-stu-id="f31bf-182">R2 measures how close the actual test data values are to the predicted values.</span></span> | <span data-ttu-id="f31bf-183">**Cuanto más cerca de 1,00, es mejor la calidad**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-183">**The closer to 1.00, the better quality**.</span></span> <span data-ttu-id="f31bf-184">Sin embargo, a veces valores bajos de R cuadrado (por ejemplo, 0,50) pueden ser completamente normales o lo suficientemente buenos en un escenario, y los valores altos de R cuadrado no siempre son buenos y pueden ser sospechosos.</span><span class="sxs-lookup"><span data-stu-id="f31bf-184">However, sometimes low R-squared values (such as 0.50) can be entirely normal or good enough for your scenario and high R-squared values are not always good and be suspicious.</span></span> |
| <span data-ttu-id="f31bf-185">**Pérdida absoluta**</span><span class="sxs-lookup"><span data-stu-id="f31bf-185">**Absolute-loss**</span></span> |  <span data-ttu-id="f31bf-186">La [pérdida absoluta](https://en.wikipedia.org/wiki/Mean_absolute_error) o la *desviación media (MAE)* mide la proximidad de las predicciones a los resultados reales.</span><span class="sxs-lookup"><span data-stu-id="f31bf-186">[Absolute-loss](https://en.wikipedia.org/wiki/Mean_absolute_error) or *Mean absolute error (MAE)* measures how close the predictions are to the actual outcomes.</span></span> <span data-ttu-id="f31bf-187">Se trata de la media de todos los errores del modelo, donde el error del modelo es la distancia absoluta entre el valor de la etiqueta predicho y el valor de la etiqueta correcto.</span><span class="sxs-lookup"><span data-stu-id="f31bf-187">It is the average of all the model errors, where model error is the absolute distance between the predicted label value and the correct label value.</span></span> <span data-ttu-id="f31bf-188">Este error de predicción se calcula para cada registro del conjunto de datos de prueba.</span><span class="sxs-lookup"><span data-stu-id="f31bf-188">This prediction error is calculated for each record of the test data set.</span></span> <span data-ttu-id="f31bf-189">Por último, el valor medio se calcula para todas las desviaciones medias registradas.</span><span class="sxs-lookup"><span data-stu-id="f31bf-189">Finally, the mean value is calculated for all recorded absolute errors.</span></span>| <span data-ttu-id="f31bf-190">**Cuanto más cerca de 0,00, es mejor la calidad.**</span><span class="sxs-lookup"><span data-stu-id="f31bf-190">**The closer to 0.00, the better quality.**</span></span> <span data-ttu-id="f31bf-191">Tenga en cuenta que la desviación media utiliza la misma escala que los datos que se van a medir (no se normaliza en un intervalo específico).</span><span class="sxs-lookup"><span data-stu-id="f31bf-191">Note that the mean absolute error uses the same scale as the data being measured (is not normalized to specific range).</span></span> <span data-ttu-id="f31bf-192">La pérdida absoluta, la pérdida cuadrática y la pérdida de RMS solo pueden usarse para realizar comparaciones entre los modelos del mismo conjunto de datos o el conjunto de datos y una distribución de valores de etiqueta similar.</span><span class="sxs-lookup"><span data-stu-id="f31bf-192">Absolute-loss, Squared-loss, and RMS-loss can only be used to make comparisons between models for the same dataset or dataset with a smilar label value distribution.</span></span> |
| <span data-ttu-id="f31bf-193">**Pérdida cuadrática**</span><span class="sxs-lookup"><span data-stu-id="f31bf-193">**Squared-loss**</span></span> |  <span data-ttu-id="f31bf-194">La [pérdida cuadrática](https://en.wikipedia.org/wiki/Mean_squared_error) o el *error cuadrático medio (MSE)* , también denominado *desviación cuadrática media (MSD)* indica la proximidad de una línea de regresión a un conjunto de valores de datos de prueba.</span><span class="sxs-lookup"><span data-stu-id="f31bf-194">[Squared-loss](https://en.wikipedia.org/wiki/Mean_squared_error) or *Mean Squared Error (MSE)*, also called *Mean Squared Deviation (MSD)*, tells you how close a regression line is to a set of test data values.</span></span> <span data-ttu-id="f31bf-195">Esto se logra midiendo las distancias desde los puntos a la línea de regresión (estas distancias son los errores E) elevándolas al cuadrado.</span><span class="sxs-lookup"><span data-stu-id="f31bf-195">It does this by taking the distances from the points to the regression line (these distances are the errors E) and squaring them.</span></span> <span data-ttu-id="f31bf-196">El cuadrado proporciona más peso a las grandes diferencias.</span><span class="sxs-lookup"><span data-stu-id="f31bf-196">The squaring gives more weight to larger differences.</span></span> | <span data-ttu-id="f31bf-197">Siempre es un valor no negativo y los **valores próximos a 0,00 son mejores**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-197">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="f31bf-198">En función de los datos, puede resultar imposible obtener un valor muy pequeño para el error cuadrático medio.</span><span class="sxs-lookup"><span data-stu-id="f31bf-198">Depending on your data, it may be impossible to get a very small value for the mean squared error.</span></span>|
| <span data-ttu-id="f31bf-199">**Pérdida de RMS**</span><span class="sxs-lookup"><span data-stu-id="f31bf-199">**RMS-loss**</span></span> |  <span data-ttu-id="f31bf-200">La [pérdida de RMS](https://en.wikipedia.org/wiki/Root-mean-square_deviation) o el *error de raíz cuadrada media (RMSE)* (también denominado *desviación de raíz cuadrada media, RMSD*), mide la diferencia entre los valores predichos por un modelo y los valores que realmente se observan en el entorno que se está modelando.</span><span class="sxs-lookup"><span data-stu-id="f31bf-200">[RMS-loss](https://en.wikipedia.org/wiki/Root-mean-square_deviation) or *Root Mean Squared Error (RMSE)* (also called *Root Mean Square Deviation, RMSD*), measures the difference between values predicted by a model and the values actually observed from the environment that is being modeled.</span></span> <span data-ttu-id="f31bf-201">La pérdida de RMS es la raíz cuadrada de la pérdida cuadrática y tiene las mismas unidades de la etiqueta, similar a la pérdida absoluta aunque proporciona más peso a las grandes diferencias.</span><span class="sxs-lookup"><span data-stu-id="f31bf-201">RMS-loss is the square root of Squared-loss and has the same units as the label, similar to the abolute-loss though giving more weight to larger diferences.</span></span> <span data-ttu-id="f31bf-202">El error de raíz cuadrada media se usa habitualmente en la climatología, la previsión y el análisis de regresión para comprobar resultados experimentales.</span><span class="sxs-lookup"><span data-stu-id="f31bf-202">Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.</span></span> | <span data-ttu-id="f31bf-203">Siempre es un valor no negativo y los **valores próximos a 0,00 son mejores**.</span><span class="sxs-lookup"><span data-stu-id="f31bf-203">It is always non-negative, and **values closer to 0.00 are better**.</span></span> <span data-ttu-id="f31bf-204">RMSD es una medida de precisión para comparar errores de previsión de diferentes modelos en determinado conjunto de datos y no entre conjuntos de datos, ya que es dependiente de la escala.</span><span class="sxs-lookup"><span data-stu-id="f31bf-204">RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.</span></span>|

<span data-ttu-id="f31bf-205">Para obtener más información sobre las métricas de regresión, lea los artículos siguientes:</span><span class="sxs-lookup"><span data-stu-id="f31bf-205">For further details on regression metrics, read the following articles:</span></span>

- [<span data-ttu-id="f31bf-206">Análisis de regresión: ¿Cómo se puede interpretar el R cuadrado y evaluar la adecuación del ajuste?</span><span class="sxs-lookup"><span data-stu-id="f31bf-206">Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?</span></span>](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)
- [<span data-ttu-id="f31bf-207">Cómo interpretar el R cuadrado en el análisis de regresión</span><span class="sxs-lookup"><span data-stu-id="f31bf-207">How To Interpret R-squared in Regression Analysis</span></span>](https://statisticsbyjim.com/regression/interpret-r-squared-regression)
- [<span data-ttu-id="f31bf-208">Definición de R cuadrado</span><span class="sxs-lookup"><span data-stu-id="f31bf-208">R-Squared Definition</span></span>](https://www.investopedia.com/terms/r/r-squared.asp)
- [<span data-ttu-id="f31bf-209">Definición de error de raíz cuadrada media</span><span class="sxs-lookup"><span data-stu-id="f31bf-209">Mean Squared Error Definition</span></span>](https://www.statisticshowto.datasciencecentral.com/mean-squared-error/)
- [<span data-ttu-id="f31bf-210">¿Qué son el error cuadrático medio y el error de raíz cuadrada media?</span><span class="sxs-lookup"><span data-stu-id="f31bf-210">What are Mean Squared Error and Root Mean Squared Error?</span></span>](https://www.vernier.com/til/1014/)
